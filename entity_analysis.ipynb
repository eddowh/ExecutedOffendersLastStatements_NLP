{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Recognition Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import feather\n",
    "import spacy\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>county</th>\n",
       "      <th>date</th>\n",
       "      <th>execution_no</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>offender_information_link</th>\n",
       "      <th>race</th>\n",
       "      <th>tdcj_no</th>\n",
       "      <th>last_statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>Bexar</td>\n",
       "      <td>2017-07-27</td>\n",
       "      <td>543</td>\n",
       "      <td>Taichin</td>\n",
       "      <td>Preyor</td>\n",
       "      <td>http://www.tdcj.state.tx.us/death_row/dr_info/...</td>\n",
       "      <td>Black</td>\n",
       "      <td>999494</td>\n",
       "      <td>First and foremost I'd like to say, \"Justice h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>Tarrant</td>\n",
       "      <td>2017-03-14</td>\n",
       "      <td>542</td>\n",
       "      <td>James</td>\n",
       "      <td>Bigby</td>\n",
       "      <td>http://www.tdcj.state.tx.us/death_row/dr_info/...</td>\n",
       "      <td>White</td>\n",
       "      <td>997</td>\n",
       "      <td>Yes, I do, Grace Kehler is that you? I have gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>Bexar</td>\n",
       "      <td>2017-03-07</td>\n",
       "      <td>541</td>\n",
       "      <td>Rolando</td>\n",
       "      <td>Ruiz</td>\n",
       "      <td>http://www.tdcj.state.tx.us/death_row/dr_info/...</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>999145</td>\n",
       "      <td>“Yes sir, I would first like to say to the San...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>2017-01-26</td>\n",
       "      <td>540</td>\n",
       "      <td>Terry</td>\n",
       "      <td>Edwards</td>\n",
       "      <td>http://www.tdcj.state.tx.us/death_row/dr_info/...</td>\n",
       "      <td>Black</td>\n",
       "      <td>999463</td>\n",
       "      <td>Yes, I made peace with God. I hope y'all make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "      <td>Tarrant</td>\n",
       "      <td>2017-01-11</td>\n",
       "      <td>539</td>\n",
       "      <td>Christopher</td>\n",
       "      <td>Wilkins</td>\n",
       "      <td>http://www.tdcj.state.tx.us/death_row/dr_info/...</td>\n",
       "      <td>White</td>\n",
       "      <td>999533</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age   county       date  execution_no   first_name last_name  \\\n",
       "0   46    Bexar 2017-07-27           543      Taichin    Preyor   \n",
       "1   61  Tarrant 2017-03-14           542        James     Bigby   \n",
       "2   44    Bexar 2017-03-07           541      Rolando      Ruiz   \n",
       "3   43   Dallas 2017-01-26           540        Terry   Edwards   \n",
       "4   48  Tarrant 2017-01-11           539  Christopher   Wilkins   \n",
       "\n",
       "                           offender_information_link      race tdcj_no  \\\n",
       "0  http://www.tdcj.state.tx.us/death_row/dr_info/...     Black  999494   \n",
       "1  http://www.tdcj.state.tx.us/death_row/dr_info/...     White     997   \n",
       "2  http://www.tdcj.state.tx.us/death_row/dr_info/...  Hispanic  999145   \n",
       "3  http://www.tdcj.state.tx.us/death_row/dr_info/...     Black  999463   \n",
       "4  http://www.tdcj.state.tx.us/death_row/dr_info/...     White  999533   \n",
       "\n",
       "                                      last_statement  \n",
       "0  First and foremost I'd like to say, \"Justice h...  \n",
       "1  Yes, I do, Grace Kehler is that you? I have gi...  \n",
       "2  “Yes sir, I would first like to say to the San...  \n",
       "3  Yes, I made peace with God. I hope y'all make ...  \n",
       "4                                               None  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in dataframe with feather\n",
    "df = feather.read_dataframe('./data/executed_offenders_last_statements.dat')\n",
    "\n",
    "# just to get a sense of what the data looks like\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this notebook is mainly about NLP, let us first examine the  `last_statements`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      First and foremost I'd like to say, \"Justice h...\n",
       "1      Yes, I do, Grace Kehler is that you? I have gi...\n",
       "2      “Yes sir, I would first like to say to the San...\n",
       "3      Yes, I made peace with God. I hope y'all make ...\n",
       "4                                                   None\n",
       "5      I don't have anything to say, you can proceed ...\n",
       "6      I just want to tell my family thank you, my mo...\n",
       "7      I would like to thank everyone that has showed...\n",
       "8      Yeah, first off, I want to say that I am sorry...\n",
       "9      To my family, to my mom, I love you. God bless...\n",
       "10                                                 None.\n",
       "11     Sending me to a better place.  I am alright wi...\n",
       "12     Yes, I would like to thank all of my supporter...\n",
       "13     Shelby, God bless your heart.  To my family, I...\n",
       "14     (Spanish) To the Solano family, I want to tell...\n",
       "15     “I would like to thank you.  I hope this execu...\n",
       "16     “Yes, I would like to thank my family and frie...\n",
       "17     \"Much has been written about this case, not al...\n",
       "18                                 I'm ready to go home.\n",
       "19     \"Thank you for being here. I am sorry for all ...\n",
       "20     “I would like to apologize to the Moreno famil...\n",
       "21     Yes sir, I want to say I love you to all my fa...\n",
       "22     That each new indignity defeats only the body....\n",
       "23     Joanna I really love and care about her, I app...\n",
       "24     There are no endings, only beginnings. Love ya...\n",
       "25     To the victim’s family, I want you to know tha...\n",
       "26     I just want to tell my family  I love them; my...\n",
       "27     Nesha, I love you. I hope this brings you some...\n",
       "28     (Written statement) \\r\\n  I always said that i...\n",
       "29     Yes, sir. First I would like to thank God for ...\n",
       "                             ...                        \n",
       "513    I would like to tell Mr. Richard that I apprec...\n",
       "514     This offender declined to make a last statement.\n",
       "515     This offender declined to make a last statement.\n",
       "516     This offender declined to make a last statement.\n",
       "517     This offender declined to make a last statement.\n",
       "518     This offender declined to make a last statement.\n",
       "519     This offender declined to make a last statement.\n",
       "520    Mother, I am sorry for all the pain I’ve cause...\n",
       "521     This offender declined to make a last statement.\n",
       "522     This offender declined to make a last statement.\n",
       "523     This offender declined to make a last statement.\n",
       "524    I want to say I’m sorry for the things I’ve do...\n",
       "525     This offender declined to make a last statement.\n",
       "526    Tell my mother I love her and continue on with...\n",
       "527    Goodbye to my family; I love all of you, I’m s...\n",
       "528                    I have no last words. I am ready.\n",
       "529    Goodbye to all my friends; be cool. Thank you ...\n",
       "530    \"Be strong for me,\" Pinkerton told his father,...\n",
       "531     This offender declined to make a last statement.\n",
       "532        I deserve this. Tell everyone I said goodbye.\n",
       "533    D.J., Laurie, Dr. Wheat, about all I can say i...\n",
       "534    I want to thank Father Walsh for his spiritual...\n",
       "535    There’s no God but Allah, and unto thy I belon...\n",
       "536     This offender declined to make a last statement.\n",
       "537    Heavenly Father, I give thanks for this time, ...\n",
       "538    I pray that my family will rejoice and will fo...\n",
       "539    When asked if he had a last statement, he repl...\n",
       "540    What is about to transpire in a few moments is...\n",
       "541     This offender declined to make a last statement.\n",
       "542    Statement to the Media: I, at this very moment...\n",
       "Name: last_statement, Length: 543, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.last_statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a quick inspection of the text we find that there are some offenders who did not have a last statement, so we will fill those with empty strings.\n",
    "\n",
    "I can identify two scenarios from this inspection:\n",
    "- `None` (the offender did not prepare a last statement)\n",
    "- `This offender declined to make a last statement.` (this is pretty self explanatory)\n",
    "\n",
    "Since the absence of last statements typically result in very short text (I'm guessing <= 50 characters), we'll filter out the long text which we can assume are definitely last statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(Mumbled.) Tell Mama I love her.',\n",
       " \"Bye, I'm Ready.\",\n",
       " 'He spoke in Irish, translating to \"Goodbye.\"',\n",
       " 'High Flight (aviation poem)',\n",
       " 'I am ready for the final blessing.',\n",
       " 'I deserve this. Tell everyone I said goodbye.',\n",
       " 'I have no last words. I am ready.',\n",
       " 'I hope Mrs. Howard can find peace in this.',\n",
       " 'I just love everybody, and that’s it.',\n",
       " \"I love ya'll and I'm gonna miss ya'll.\",\n",
       " 'I love you Israel.',\n",
       " 'I love you, Mom. Goodbye.',\n",
       " 'I wish everybody a good life. Everything is O.K.',\n",
       " \"I'll see you.\",\n",
       " \"I'm ready to go home.\",\n",
       " 'I’m ready to be released. Release me.',\n",
       " 'I’m ready, Warden.',\n",
       " \"Love ya'll, see you on the other side.\",\n",
       " 'No',\n",
       " 'No last statement.',\n",
       " 'No, I have no final statement.',\n",
       " 'None',\n",
       " 'None.',\n",
       " 'Peace.',\n",
       " 'Profanity directed toward staff.',\n",
       " 'Santajaib Singh Ji.',\n",
       " 'Thanked his family.',\n",
       " 'There’s love and peace in Islam.',\n",
       " 'This offender declined to make a last statement.',\n",
       " 'Well, my friends in my heart, I’m ready –',\n",
       " \"Yes, Ain't no way fo' fo', I Love all yall.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out text longer than 50 characters\n",
    "short_last_statements = [\n",
    "    ls for ls in df.last_statement\n",
    "    if len(ls) <= 50\n",
    "]\n",
    "\n",
    "# unique short last statements\n",
    "set(short_last_statements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can see that there are a couple scenarios that could be identified as an absence of a last statement:\n",
    "- No\n",
    "- No last statement.\n",
    "- No, I have no final statement.\n",
    "- None\n",
    "- None.\n",
    "- This offender declined to make a last statement.\n",
    "\n",
    "All of these could be classified as an absence of a last statement, however, I will choose to consider *\"No last statement.\"* and *\"No, I have no final statement.\"* as last statements themselves, seeing that it is possible the offenders were going for a witty remark about having no last statements -- but that's still a last statement in my book!\n",
    "\n",
    "Now onto empty strings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSCENCE_OF_LAST_STATEMENTS = {\n",
    "    'No',\n",
    "    'None',\n",
    "    'None.',\n",
    "    'This offender declined to make a last statement.',\n",
    "}\n",
    "\n",
    "\n",
    "df.last_statement = df.last_statement.apply(\n",
    "    lambda t: '' if t in ABSCENCE_OF_LAST_STATEMENTS else t\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's mentioned the most?\n",
    "\n",
    "One of the most common analysis in NLP involves finding the most common words in the text. However, with spacy's out-of-the-box NER (Named Entity Recognition) tool, I'm more interested in seeing what entities are being mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Sensitivity in NER\n",
    "\n",
    "There is one thing to note: it is common practice in NLP to transform all text to lowercase for case insentitivity; however, spacy's NER apparently relies on case sensitivity.\n",
    "\n",
    "Below is a quick demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE - United States\n"
     ]
    }
   ],
   "source": [
    "# list out all entites for the text 'United States'\n",
    "for ent in nlp('United States').ents:\n",
    "    print(\"{} - {}\".format(ent.label_, ent.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: **GPE** means **Geo-Political Entity**. A full list of built-in entity types from spacy's NER can be found in  [spacy's documentation on Entity recognition](https://spacy.io/docs/usage/entity-recognition#entity-types).*\n",
    "\n",
    "Now what happens if we tell spacy to identify entities in the text \"united states\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list out all entities for the text 'united states'\n",
    "for ent in nlp('united states').ents:\n",
    "    print(\"{} - {}\", ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no output because there are no entities recognized. Here's another example with \"UNITED STATES\" (all caps):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list out all entities for the text 'UNITED STATES'\n",
    "for ent in nlp('UNITED STATES').ents:\n",
    "    print(\"{} - {}\", ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, no entities are recognized for \"UNITED STATES\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Entity Recognition Transformation\n",
    "\n",
    "Now that we know that case sensitivity matters when recognizing entities, we will have to preserve the text in its original form. Only when spacy successfully identifies an entity do we transform the text by:\n",
    "- lowercasing all characters\n",
    "- stripping leading and trailing space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ORG', 'GPE', 'LANGUAGE', 'CARDINAL', 'LOC', 'PERSON', 'EVENT', 'TIME', 'DATE', 'MONEY', 'QUANTITY', 'LAW', 'FAC', 'NORP', 'WORK_OF_ART', 'ORDINAL', 'PRODUCT'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analyze entities\n",
    "all_entities = defaultdict(list)\n",
    "\n",
    "for last_statement in df.last_statement:\n",
    "    parsed_ls = nlp(last_statement)\n",
    "    for entity in parsed_ls.ents:\n",
    "        # organize each entity identified with a label, using a dictionary\n",
    "        et = entity.text\n",
    "        # remove punctuations and stopwords\n",
    "        if not (nlp.vocab[et].is_punct or nlp.vocab[et].is_space):\n",
    "            # transform entity text here\n",
    "            all_entities[entity.label_].append(et.lower().strip())\n",
    "            \n",
    "# print all keys of `all_entities`:\n",
    "all_entities.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample of persons identified in all the last statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coretta scott king',\n",
       " 'crain',\n",
       " 'grace kehler',\n",
       " 'jesus christ',\n",
       " 'johnson',\n",
       " 'jones',\n",
       " 'kehler',\n",
       " 'lord',\n",
       " 'lord jesus',\n",
       " 'sanchez',\n",
       " 'warden',\n",
       " 'warden jones'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(all_entities['PERSON'][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us observe the most popular words for each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG\n",
      "  - lord (11)\n",
      "  - christ (9)\n",
      "  - the state of texas (7)\n",
      "  - mama (6)\n",
      "  - jesus (5)\n",
      "\n",
      "GPE\n",
      "  - allah (40)\n",
      "  - texas (18)\n",
      "  - warden (18)\n",
      "  - america (13)\n",
      "  - heaven (10)\n",
      "\n",
      "LANGUAGE\n",
      "  - english (6)\n",
      "  - french (1)\n",
      "  - chapter (1)\n",
      "  - spanish (1)\n",
      "\n",
      "CARDINAL\n",
      "  - one (54)\n",
      "  - two (16)\n",
      "  - three (10)\n",
      "  - 1 (3)\n",
      "  - zero (2)\n",
      "\n",
      "LOC\n",
      "  - earth (5)\n",
      "  - west (2)\n",
      "  - christ jesus (2)\n",
      "  - north (1)\n",
      "  - north america (1)\n",
      "\n",
      "PERSON\n",
      "  - warden (72)\n",
      "  - lord (44)\n",
      "  - father (38)\n",
      "  - jesus christ (17)\n",
      "  - irene (15)\n",
      "\n",
      "EVENT\n",
      "  - my cup (1)\n",
      "\n",
      "TIME\n",
      "  - tonight (36)\n",
      "  - night (4)\n",
      "  - hour (1)\n",
      "  - 3 minutes (1)\n",
      "  - a few minutes (1)\n",
      "\n",
      "DATE\n",
      "  - today (47)\n",
      "  - ya'll (36)\n",
      "  - the years (16)\n",
      "  - this day (9)\n",
      "  - ya’ll (6)\n",
      "\n",
      "MONEY\n",
      "  - as much as (1)\n",
      "  - 903 (1)\n",
      "\n",
      "QUANTITY\n",
      "  - 180 pounds (1)\n",
      "\n",
      "LAW\n",
      "  - article 19.83 of the texas penal code of (1)\n",
      "\n",
      "FAC\n",
      "  - warden okay (1)\n",
      "  - st. thomas (1)\n",
      "  - ya'll (1)\n",
      "\n",
      "NORP\n",
      "  - spanish (9)\n",
      "  - christ (5)\n",
      "  - american (5)\n",
      "  - romans (5)\n",
      "  - christian (4)\n",
      "\n",
      "WORK_OF_ART\n",
      "  - love (15)\n",
      "  - bye aunt helen, luise, joanna and (1)\n",
      "  - seeing through the eyes of a death row inmate (1)\n",
      "  - o.k. i am (1)\n",
      "  - i (1)\n",
      "\n",
      "ORDINAL\n",
      "  - first (53)\n",
      "  - second (3)\n",
      "  - son (1)\n",
      "  - 10th (1)\n",
      "  - secondly (1)\n",
      "\n",
      "PRODUCT\n",
      "  - chaplain (1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_common_entities = defaultdict(Counter)\n",
    "for entity_label, entity_names in all_entities.items():\n",
    "    most_common_entities[entity_label] = Counter(entity_names)\n",
    "    # for printing purposes\n",
    "    print(entity_label)\n",
    "    for common_entity in most_common_entities[entity_label].most_common(5):\n",
    "        entity_name, count = common_entity\n",
    "        print(\"  - {} ({})\".format(entity_name, count))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misclassification of Entities\n",
    "\n",
    "We can see that certain entities are sometimes grouped into different categories. Usually, this makes sense because \"French\" can either be interpreted as `NORP`, `LANGUAGE`, or `PERSON` (i.e. the French).\n",
    "\n",
    "However, note the following:\n",
    "- \"jesus christ\" was categorized as `PERSON`\n",
    "- \"christ\" and \"jesus\" as separate terms were categorized as `ORG`\n",
    "- \"christ jesus\" was categorized as `LOC` (location).\n",
    "\n",
    "There are more examples, such as:\n",
    "- \"warden okay\" being categorized as `FAC` (facility)\n",
    "- \"ya'll\" being categorized as `FAC` and `DATE`\n",
    "\n",
    "This is seen as a shortcoming of spacy's NER (Named Entity Recognition), so it might be more useful to just see the most common entities across all text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('warden', 90),\n",
       " ('lord', 55),\n",
       " ('one', 54),\n",
       " ('first', 53),\n",
       " ('today', 47),\n",
       " (\"ya'll\", 43),\n",
       " ('allah', 42),\n",
       " ('father', 38),\n",
       " ('tonight', 36),\n",
       " ('jesus', 28),\n",
       " ('jesus christ', 21),\n",
       " ('texas', 18),\n",
       " ('christ', 18),\n",
       " ('love', 17),\n",
       " ('two', 16),\n",
       " ('the years', 16),\n",
       " ('irene', 15),\n",
       " ('lord jesus', 13),\n",
       " ('america', 13),\n",
       " ('heaven', 10),\n",
       " ('three', 10),\n",
       " ('god', 10),\n",
       " ('spanish', 10),\n",
       " ('jack', 10),\n",
       " ('mama', 9)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten all words from defaultdict.values\n",
    "all_words = [word for label in list(all_entities.values()) for word in label]\n",
    "bag_of_words = Counter(all_words)\n",
    "\n",
    "# display the most common 25 entities mentioned\n",
    "bag_of_words.most_common(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
