{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import feather\n",
    "import spacy\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>county</th>\n",
       "      <th>date</th>\n",
       "      <th>execution_no</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>offender_information_link</th>\n",
       "      <th>race</th>\n",
       "      <th>tdcj_no</th>\n",
       "      <th>last_statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>Bexar</td>\n",
       "      <td>2017-07-27</td>\n",
       "      <td>543</td>\n",
       "      <td>Taichin</td>\n",
       "      <td>Preyor</td>\n",
       "      <td>http://www.tdcj.state.tx.us/death_row/dr_info/...</td>\n",
       "      <td>Black</td>\n",
       "      <td>999494</td>\n",
       "      <td>First and foremost I'd like to say, \"Justice h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>Tarrant</td>\n",
       "      <td>2017-03-14</td>\n",
       "      <td>542</td>\n",
       "      <td>James</td>\n",
       "      <td>Bigby</td>\n",
       "      <td>http://www.tdcj.state.tx.us/death_row/dr_info/...</td>\n",
       "      <td>White</td>\n",
       "      <td>997</td>\n",
       "      <td>Yes, I do, Grace Kehler is that you? I have gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>Bexar</td>\n",
       "      <td>2017-03-07</td>\n",
       "      <td>541</td>\n",
       "      <td>Rolando</td>\n",
       "      <td>Ruiz</td>\n",
       "      <td>http://www.tdcj.state.tx.us/death_row/dr_info/...</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>999145</td>\n",
       "      <td>“Yes sir, I would first like to say to the San...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>2017-01-26</td>\n",
       "      <td>540</td>\n",
       "      <td>Terry</td>\n",
       "      <td>Edwards</td>\n",
       "      <td>http://www.tdcj.state.tx.us/death_row/dr_info/...</td>\n",
       "      <td>Black</td>\n",
       "      <td>999463</td>\n",
       "      <td>Yes, I made peace with God. I hope y'all make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "      <td>Tarrant</td>\n",
       "      <td>2017-01-11</td>\n",
       "      <td>539</td>\n",
       "      <td>Christopher</td>\n",
       "      <td>Wilkins</td>\n",
       "      <td>http://www.tdcj.state.tx.us/death_row/dr_info/...</td>\n",
       "      <td>White</td>\n",
       "      <td>999533</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age   county       date  execution_no   first_name last_name  \\\n",
       "0   46    Bexar 2017-07-27           543      Taichin    Preyor   \n",
       "1   61  Tarrant 2017-03-14           542        James     Bigby   \n",
       "2   44    Bexar 2017-03-07           541      Rolando      Ruiz   \n",
       "3   43   Dallas 2017-01-26           540        Terry   Edwards   \n",
       "4   48  Tarrant 2017-01-11           539  Christopher   Wilkins   \n",
       "\n",
       "                           offender_information_link      race tdcj_no  \\\n",
       "0  http://www.tdcj.state.tx.us/death_row/dr_info/...     Black  999494   \n",
       "1  http://www.tdcj.state.tx.us/death_row/dr_info/...     White     997   \n",
       "2  http://www.tdcj.state.tx.us/death_row/dr_info/...  Hispanic  999145   \n",
       "3  http://www.tdcj.state.tx.us/death_row/dr_info/...     Black  999463   \n",
       "4  http://www.tdcj.state.tx.us/death_row/dr_info/...     White  999533   \n",
       "\n",
       "                                      last_statement  \n",
       "0  First and foremost I'd like to say, \"Justice h...  \n",
       "1  Yes, I do, Grace Kehler is that you? I have gi...  \n",
       "2  “Yes sir, I would first like to say to the San...  \n",
       "3  Yes, I made peace with God. I hope y'all make ...  \n",
       "4                                               None  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in dataframe with feather\n",
    "df = feather.read_dataframe('./data/executed_offenders_last_statements.dat')\n",
    "\n",
    "# just to get a sense of what the data looks like\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's mentioned the most?\n",
    "\n",
    "One of the most common analysis in NLP is to see the most common words found in the text. However, with spacy's out-of-the-box NER (Named Entity Recognition) tool, I'm more interested in seeing what entities are being mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Sensitivity in NER\n",
    "\n",
    "There is one thing to note: it is common practice in NLP to transform all text to lowercase for case insentitivity; however, spacy's NER apparently relies on case sensitivity.\n",
    "\n",
    "Below is a quick demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE - United States\n"
     ]
    }
   ],
   "source": [
    "# list out all entites for the text 'United States'\n",
    "for ent in nlp('United States').ents:\n",
    "    print(\"{} - {}\".format(ent.label_, ent.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: **GPE** means **Geo-Political Entity**. A full list of built-in entity types from spacy's NER can be found in  [spacy's documentation on Entity recognition](https://spacy.io/docs/usage/entity-recognition#entity-types).*\n",
    "\n",
    "Now what happens if we tell spacy to identify entities in the text \"united states\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list out all entities for the text 'united states'\n",
    "for ent in nlp('united states').ents:\n",
    "    print(\"{} - {}\", ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no output because there are no entities recognized. Here's another example with \"UNITED STATES\" (all caps):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list out all entities for the text 'UNITED STATES'\n",
    "for ent in nlp('UNITED STATES').ents:\n",
    "    print(\"{} - {}\", ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, no entities are recognized for \"UNITED STATES\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Entity Recognition Transformation\n",
    "\n",
    "Now that we know that case sensitivity matters when recognizing entities, we will have to preserve the text in its original form. Only when spacy successfully identifies an entity do we transform the text by:\n",
    "- lowercasing all characters\n",
    "- stripping leading and trailing space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['EVENT', 'CARDINAL', 'NORP', 'LAW', 'QUANTITY', 'GPE', 'PRODUCT', 'TIME', 'MONEY', 'WORK_OF_ART', 'LANGUAGE', 'PERSON', 'ORG', 'FAC', 'DATE', 'ORDINAL', 'LOC'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analyze entities\n",
    "all_entities = defaultdict(list)\n",
    "\n",
    "for last_statement in df.last_statement:\n",
    "    parsed_ls = nlp(last_statement)\n",
    "    for entity in parsed_ls.ents:\n",
    "        # organize each entity identified with a label, using a dictionary\n",
    "        et = entity.text\n",
    "        # remove punctuations and stopwords\n",
    "        if not (nlp.vocab[et].is_punct or nlp.vocab[et].is_space):\n",
    "            # transform entity text here\n",
    "            all_entities[entity.label_].append(et.lower().strip())\n",
    "            \n",
    "# print all keys of `all_entities`:\n",
    "all_entities.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample of persons identified in all the last statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coretta scott king',\n",
       " 'crain',\n",
       " 'grace kehler',\n",
       " 'jesus christ',\n",
       " 'johnson',\n",
       " 'jones',\n",
       " 'kehler',\n",
       " 'lord',\n",
       " 'lord jesus',\n",
       " 'sanchez',\n",
       " 'warden',\n",
       " 'warden jones'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(all_entities['PERSON'][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us observe the most popular words for each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT\n",
      "  - my cup (1)\n",
      "\n",
      "CARDINAL\n",
      "  - one (54)\n",
      "  - two (16)\n",
      "  - three (10)\n",
      "  - 1 (3)\n",
      "  - zero (2)\n",
      "\n",
      "NORP\n",
      "  - spanish (9)\n",
      "  - romans (5)\n",
      "  - american (5)\n",
      "  - christ (5)\n",
      "  - christian (4)\n",
      "\n",
      "LAW\n",
      "  - article 19.83 of the texas penal code of (1)\n",
      "\n",
      "QUANTITY\n",
      "  - 180 pounds (1)\n",
      "\n",
      "GPE\n",
      "  - allah (40)\n",
      "  - warden (18)\n",
      "  - texas (18)\n",
      "  - america (13)\n",
      "  - heaven (10)\n",
      "\n",
      "PRODUCT\n",
      "  - chaplain (1)\n",
      "\n",
      "TIME\n",
      "  - tonight (36)\n",
      "  - night (4)\n",
      "  - the night of nov 11? (1)\n",
      "  - goodnight (1)\n",
      "  - 13:13 (1)\n",
      "\n",
      "MONEY\n",
      "  - as much as (1)\n",
      "  - 903 (1)\n",
      "\n",
      "WORK_OF_ART\n",
      "  - love (15)\n",
      "  - god living with us 24 hours (1)\n",
      "  - the (1)\n",
      "  - dear heavenly father (1)\n",
      "  - bye aunt helen, luise, joanna and (1)\n",
      "\n",
      "LANGUAGE\n",
      "  - english (6)\n",
      "  - french (1)\n",
      "  - spanish (1)\n",
      "  - chapter (1)\n",
      "\n",
      "PERSON\n",
      "  - warden (72)\n",
      "  - lord (44)\n",
      "  - father (38)\n",
      "  - jesus christ (17)\n",
      "  - irene (15)\n",
      "\n",
      "ORG\n",
      "  - lord (11)\n",
      "  - christ (9)\n",
      "  - the state of texas (7)\n",
      "  - mama (6)\n",
      "  - jesus (5)\n",
      "\n",
      "FAC\n",
      "  - ya'll (1)\n",
      "  - st. thomas (1)\n",
      "  - warden okay (1)\n",
      "\n",
      "DATE\n",
      "  - today (47)\n",
      "  - ya'll (36)\n",
      "  - the years (16)\n",
      "  - this day (9)\n",
      "  - ya’ll (6)\n",
      "\n",
      "ORDINAL\n",
      "  - last (109)\n",
      "  - first (53)\n",
      "  - second (3)\n",
      "  - 10th (1)\n",
      "  - 1st (1)\n",
      "\n",
      "LOC\n",
      "  - earth (5)\n",
      "  - west (2)\n",
      "  - christ jesus (2)\n",
      "  - east (1)\n",
      "  - north (1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_common_entities = defaultdict(Counter)\n",
    "for entity_label, entity_names in all_entities.items():\n",
    "    most_common_entities[entity_label] = Counter(entity_names)\n",
    "    # for printing purposes\n",
    "    print(entity_label)\n",
    "    for common_entity in most_common_entities[entity_label].most_common(5):\n",
    "        entity_name, count = common_entity\n",
    "        print(\"  - {} ({})\".format(entity_name, count))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Misclassification of Entities\n",
    "\n",
    "We can see that certain entities are sometimes grouped into different categories. Usually, this makes sense because \"French\" can either be interpreted as `NORP`, `LANGUAGE`, or `PERSON` (i.e. the French).\n",
    "\n",
    "However, note the following:\n",
    "- \"jesus christ\" was categorized as `PERSON`\n",
    "- \"christ\" and \"jesus\" as separate terms were categorized as `ORG`\n",
    "- \"christ jesus\" was categorized as `LOC` (location).\n",
    "\n",
    "This is seen as a shortcoming of spacy's NER (Named Entity Recognition), so it might be more useful to just see the most common entities across the entire text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('last', 109),\n",
       " ('warden', 90),\n",
       " ('lord', 55),\n",
       " ('one', 54),\n",
       " ('first', 53),\n",
       " ('today', 47),\n",
       " (\"ya'll\", 43),\n",
       " ('allah', 42),\n",
       " ('father', 38),\n",
       " ('tonight', 36),\n",
       " ('jesus', 28),\n",
       " ('jesus christ', 21),\n",
       " ('texas', 18),\n",
       " ('christ', 18),\n",
       " ('love', 17),\n",
       " ('two', 16),\n",
       " ('the years', 16),\n",
       " ('irene', 15),\n",
       " ('america', 13),\n",
       " ('lord jesus', 13),\n",
       " ('god', 10),\n",
       " ('heaven', 10),\n",
       " ('jack', 10),\n",
       " ('three', 10),\n",
       " ('spanish', 10)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten all words from defaultdict.values\n",
    "all_words = [word for label in list(all_entities.values()) for word in label]\n",
    "bag_of_words = Counter(all_words)\n",
    "\n",
    "# display the most common 25 entities mentioned\n",
    "bag_of_words.most_common(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
